{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e1082cda-7a1c-4c06-b03b-914c4057ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "from matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage,\n",
    "                                  AnnotationBbox)\n",
    "from operator import add\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tensorflow as tf\n",
    "from IPython import embed\n",
    "from gym import wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "48d66e81-5ab3-4e18-81e4-68215e7ab59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14d9d1fd0>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "action_space=[0,1]\n",
    "num_states = env.observation_space.shape[0]\n",
    "done = False\n",
    "\n",
    "seed = 6\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# while not done:\n",
    "# #     env.render()\n",
    "\n",
    "#     observation, reward, done, info = env.step(env.action_space.sample())\n",
    "#     if done:\n",
    "#         reward = 0\n",
    "#     print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2ab92437-5404-4e61-980b-f46cb04efbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class act_Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(act_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "#         self.fc2 = nn.Linear(24,12)\n",
    "        self.fc3 = nn.Linear(64,output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c5d06747-dfb4-40ee-bc29-70838b7b560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic_Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(critic_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "#         self.fc2 = nn.Linear(24,12)\n",
    "        self.fc3 = nn.Linear(64,output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a4b4251b-df45-4bcc-b58e-bae6bda8cf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]\n",
    "# env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "02caec97-d5fe-426e-a482-332131ebc31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_class():\n",
    "    def __init__(self,action_state, output_state):     \n",
    "        self.network = act_Net(action_state, output_state)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=0.001)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "186e3101-c031-40dd-a95d-5c81ffc9295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic_class():\n",
    "    def __init__(self,action_state, output_state): \n",
    "        self.network = critic_Net(action_state, output_state)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=0.001)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3670944c-865c-45da-8f3c-8bb85192aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "16cd1617-25ae-4839-ac76-46d5cf77083c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actor = actor_class(env.observation_space.shape[0], env.action_space.n)\n",
    "critic = critic_class(env.observation_space.shape[0], 1)\n",
    "class A2C():\n",
    "    def __init__(self, num_episodes = 1000):\n",
    "        \n",
    "        self.number_of_episodes = num_episodes\n",
    "        self.hyper_parameters = {}\n",
    "        self.hyper_parameters['learning_rate'] = 0.001\n",
    "        self.hyper_parameters['max_timesteps'] = 100\n",
    "        self.hyper_parameters['epsilon'] = 1\n",
    "        self.hyper_parameters['epsilon_decay'] = 1.01010101\n",
    "        self.hyper_parameters['discount_factor'] = 0.99\n",
    "        self.hyper_parameters['replay_length'] = 2000\n",
    "        self.hyper_parameters['batch_size'] = 32\n",
    "        \n",
    "        \n",
    "    def encode_one_hot(self, action, n_actions):\n",
    "        encoded = np.zeros(n_actions, np.float32)\n",
    "        encoded[action] = 1\n",
    "        return encoded\n",
    "\n",
    "    def train(self):       \n",
    "        metrics['total_rewards'] = []\n",
    "        for episode in range(self.number_of_episodes):\n",
    "            total_training_rewards = 0\n",
    "            curr_state = env.reset()\n",
    "            curr_state = curr_state.reshape([1, curr_state.shape[0]])\n",
    "            done = False\n",
    "            while not done:\n",
    "\n",
    "                act_probabilities = actor.network(torch.Tensor(curr_state))#.detach().numpy()[0]\n",
    "                dist = torch.distributions.Categorical(probs=act_probabilities)\n",
    "                action = dist.sample()\n",
    "#                 action = np.random.choice(env.action_space.n, 1, p=act_probabilities)[0]\n",
    "                encoded_action = self.encode_one_hot(action, env.action_space.n)\n",
    "\n",
    "                next_state, reward, done, info = env.step(action.detach().numpy()[0])\n",
    "                next_state = next_state.reshape([1, next_state.shape[0]])\n",
    "\n",
    "                value_curr = critic.network(torch.Tensor(curr_state))\n",
    "                value_next = critic.network(torch.Tensor(next_state))\n",
    "\n",
    "\n",
    "                TD_target = reward + (1 - done) * self.hyper_parameters['discount_factor'] * value_next\n",
    "                advantage = TD_target - value_curr\n",
    "\n",
    "                critic_loss = advantage.pow(2).mean()\n",
    "                critic.optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                critic.optimizer.step()\n",
    "                \n",
    "#                 target = act_probabilities[action]*advantage\n",
    "                actor_loss = -dist.log_prob(action)*advantage.detach()\n",
    "#                 actor_loss = actor.loss(torch.Tensor([-act_probabilities[action]]), torch.Tensor([-target]))\n",
    "                actor.optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor.optimizer.step()\n",
    "                                                  \n",
    "                curr_state = next_state\n",
    "                total_training_rewards += 1\n",
    "\n",
    "                if done:\n",
    "                    metrics['total_rewards'].append(total_training_rewards)\n",
    "                    if episode%100 == 99:       \n",
    "                        print(episode, metrics['total_rewards'][-10:])\n",
    "            if len(metrics['total_rewards'])>200:\n",
    "                count = 0\n",
    "                for tr in metrics['total_rewards'][-60:]:\n",
    "                    if tr>470:\n",
    "                        count += 1\n",
    "                if count>50:\n",
    "                    print(count)\n",
    "                    break\n",
    "                        \n",
    "                        \n",
    "#                     actor.save_weights(actor_checkpoint_path)\n",
    "#                     critic.save_weights(critic_checkpoint_path)\n",
    "    def execute(self,num_episodes):\n",
    "#         env = gym.make('CartPole-v1')\n",
    "    #     env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "        metrics['rewards_evaluation'] = []\n",
    "        for episodes in range(num_episodes):\n",
    "            step_count = 0\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                act_probabilities = actor.network(torch.Tensor(state)).detach().numpy()\n",
    "                action = np.argmax(act_probabilities)\n",
    "#                 print(action, act_probabilities)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                if done:\n",
    "                    reward = 0\n",
    "#                 print(action,step_count)\n",
    "    #                 env.render()\n",
    "                step_count += 1\n",
    "            print(step_count)\n",
    "            print(\"---------------end-----------------\")\n",
    "            metrics['rewards_evaluation'].append(step_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "15b48c62-a774-4b66-ad59-0cc853b1953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/kksgfjf13x3g8sx7lr4h1ts00000gn/T/ipykernel_4854/1606409619.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 [40, 60, 59, 50, 41, 60, 33, 97, 30, 35]\n",
      "199 [121, 103, 150, 57, 75, 93, 75, 44, 70, 69]\n",
      "299 [37, 30, 42, 30, 36, 34, 31, 31, 28, 28]\n",
      "399 [38, 45, 32, 37, 40, 34, 29, 26, 30, 39]\n",
      "499 [224, 239, 160, 160, 126, 355, 170, 187, 214, 191]\n",
      "599 [173, 221, 235, 187, 228, 225, 212, 145, 169, 181]\n",
      "699 [184, 178, 115, 117, 110, 93, 156, 112, 319, 111]\n",
      "799 [234, 500, 500, 335, 500, 405, 406, 500, 262, 192]\n",
      "899 [500, 500, 500, 458, 396, 378, 500, 500, 500, 500]\n",
      "51\n",
      "500\n",
      "---------------end-----------------\n",
      "500\n",
      "---------------end-----------------\n",
      "500\n",
      "---------------end-----------------\n",
      "500\n",
      "---------------end-----------------\n",
      "500\n",
      "---------------end-----------------\n",
      "500\n",
      "---------------end-----------------\n",
      "500\n",
      "---------------end-----------------\n",
      "500\n",
      "---------------end-----------------\n",
      "500\n",
      "---------------end-----------------\n",
      "500\n",
      "---------------end-----------------\n"
     ]
    }
   ],
   "source": [
    "ac2 = A2C(3000)\n",
    "ac2.train()\n",
    "ac2.execute(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8dfa1471-ce52-4b7c-af9d-57ac2a27a31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Figure size 432x288 with 1 Axes>\n",
      "<Figure size 432x288 with 1 Axes>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(metrics['total_rewards'])\n",
    "plt.figtext(0.5, 0.01, \"Total Rewards per episode\", wrap=True, horizontalalignment='center', fontsize=12)\n",
    "plt.savefig('total_rewards.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(metrics['rewards_evaluation'])\n",
    "plt.figtext(0.5, 0.01, \"Rewards evaluation on test set\", wrap=True, horizontalalignment='center', fontsize=12)\n",
    "plt.savefig('rewards evaluation.png')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d7011-2c01-450c-ab9d-13c15f38df0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee68d57-e637-4626-a8d0-9f18033f0cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
